{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2Tcc-MFPNqe"
   },
   "source": [
    "cell 1: Setup: Imports & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using PyFIM fpgrowth; 'maximal' mode will post-filter to FP-Max.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "def _ensure(pkg, import_name=None, pip_name=None):\n",
    "    name = import_name or pkg\n",
    "    try:\n",
    "        return importlib.import_module(name)\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", pip_name or pkg])\n",
    "        return importlib.import_module(name)\n",
    "\n",
    "yaml = _ensure(\"pyyaml\", \"yaml\", \"pyyaml\")\n",
    "np   = _ensure(\"numpy\", \"numpy\", \"numpy\")\n",
    "pd   = _ensure(\"pandas\", \"pandas\", \"pandas\")\n",
    "tqdm = _ensure(\"tqdm\", \"tqdm\", \"tqdm\")\n",
    "fim  = _ensure(\"pyfim\", \"fim\", \"pyfim==6.28\")   # provides fpgrowth (fpmax is NOT exposed)\n",
    "\n",
    "# --- configuration ---\n",
    "import os, json, math, glob, itertools\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Any, Dict, Iterable, List, Optional, Set, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset location\n",
    "DATA_DIR = Path(\"/Users/ankita/Desktop/V3_DatasetTest_Valid\")\n",
    "GLOB_PATTERN = \"**/*.Pipeline\"\n",
    "\n",
    "# Mining thresholds\n",
    "MIN_SUPPORT_RATIO = 0.10    # appear in >=20% of files\n",
    "MIN_ITEMSET_LEN   = 1       # set 2 to skip singletons\n",
    "MAX_ITEMSET_LEN   = 10\n",
    "\n",
    "# Feature extraction knobs\n",
    "LOWERCASE_VALUES        = True\n",
    "MAX_VALUE_LEN           = 120\n",
    "KEEP_NUMERIC_VALUES     = True\n",
    "INCLUDE_EXISTS_FEATURES = True\n",
    "INCLUDE_EQ_FEATURES     = True\n",
    "WILDCARD = \"[]\"          # list wildcard\n",
    "\n",
    "# Vocabulary pruning\n",
    "MIN_DOC_FREQ       = 50      # MIN_DOC_FREQ - Keep only features that appear in at least this many documents\n",
    "MAX_DOC_FREQ_RATIO = 0.98    # MAX_DOC_FREQ_RATIO - Drop features that appear in more than this fraction of documents\n",
    "TOP_K_FEATURES     = None\n",
    "\n",
    "# Verification/outputs\n",
    "BATCH_VERIFY = 500\n",
    "SAVE_DIR = Path(\"/Users/ankita/Desktop/Thesis-work/pattern_outputs2\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Choose: \"all\" (frequent itemsets), \"maximal\" (post-filter), or \"closed\" (post-filter)\n",
    "MINING_MODE = \"all\"   # options: \"all\" | \"maximal\" | \"closed\"\n",
    "\n",
    "\n",
    "print(\"Setup complete. Using PyFIM fpgrowth; 'maximal' mode will post-filter to FP-Max.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2Tcc-MFPNqe"
   },
   "source": [
    "cell 2: Uncomment & Run this cell - When re-running on new dataset to delete existing data files from previous runs so it doesnt interefere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "for p in (SAVE_DIR/\"verified_patterns_summary.csv\",\n",
    "          SAVE_DIR/\"verified_patterns_maximal.jsonl\"):\n",
    "    try: os.remove(p)\n",
    "    except FileNotFoundError: pass\n",
    "for f in SAVE_DIR.glob(\"verified_patterns_part_*.jsonl\"):\n",
    "    f.unlink()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-fi1HZtPcqZ"
   },
   "source": [
    "Cell 3 — Discover files & sanity preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Wb6SACj1Pc2W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered .Pipeline files: 29,069\n",
      "> /Users/ankita/Desktop/V3_DatasetTest_Valid/0-vortex_github-actions-dependent-jobs-example_contents_.github_workflows_deploy.Pipeline\n",
      "> /Users/ankita/Desktop/V3_DatasetTest_Valid/0003c660988cf730c6feedcce9806ef3c4432cf0.Pipeline\n",
      "> /Users/ankita/Desktop/V3_DatasetTest_Valid/00143f50002a05b8faad1fbb93ce3e0d85bde964.Pipeline\n",
      "> /Users/ankita/Desktop/V3_DatasetTest_Valid/002e6e41de6f334b7fdd2714979028ea118b1ff1.Pipeline\n",
      "> /Users/ankita/Desktop/V3_DatasetTest_Valid/006imran_event-dispatcher-workflows_contents_.github_workflows_push.Pipeline\n",
      "\n",
      "--- Preview: /Users/ankita/Desktop/V3_DatasetTest_Valid/0-vortex_github-actions-dependent-jobs-example_contents_.github_workflows_deploy.Pipeline ---\n",
      "name: Node CI/CD\n",
      "on:\n",
      "  push:\n",
      "    branches: [main]\n",
      "  pull_request:\n",
      "jobs:\n",
      "  build:\n",
      "    runs-on: ubuntu-latest\n",
      "    steps:\n",
      "    - name: \"Checkout repository\"\n",
      "      uses: actions/checkout@v2\n",
      "    - name: \"Setup Node\"\n",
      "      uses: actions/setup-node@v2\n",
      "      with:\n",
      "        node-version: 16.x\n",
      "    - name: \"Install npm@7\"\n",
      "      run: \"npm i -g npm@7\"\n",
      "    - name: \"Install dependencies\"\n",
      "      run: \"npm ci\\nnpm audit --production\\nnpm test\\n\"\n",
      "    - name: \"Release\"\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Discover files & read\n",
    "\n",
    "all_files = sorted(DATA_DIR.glob(GLOB_PATTERN))\n",
    "print(f\"Discovered .Pipeline files: {len(all_files):,}\")\n",
    "for p in all_files[:5]:\n",
    "    print(\">\", p)\n",
    "\n",
    "def peek_text(p: Path, n=20):\n",
    "    try:\n",
    "        with p.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for i, line in zip(range(n), f):\n",
    "                print(line.rstrip())\n",
    "    except Exception as e:\n",
    "        print(\"read error:\", e)\n",
    "\n",
    "if all_files:\n",
    "    print(\"\\n--- Preview:\", all_files[0], \"---\")\n",
    "    peek_text(all_files[0], n=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mlz0lumPPdCk"
   },
   "source": [
    "Cell 4 — YAML parsing + feature extraction (with list wildcards)\n",
    "\n",
    "Feature scheme\n",
    "\n",
    "exists:path for any mapping key / scalar path (e.g., exists:on.push, exists:jobs[].runs-on)\n",
    "\n",
    "eq:path==value for scalar equality (e.g., eq:jobs[].runs-on==ubuntu-latest)\n",
    "\n",
    "Lists become [] in the path to indicate any index (wildcard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from 29,069 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|███████████████████████| 29069/29069 [00:12<00:00, 2418.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extraction summary [processes (n=7)]:\n",
      "  Total files       : 29,069\n",
      "  Non-empty feature : 29,069\n",
      "  Empty feature     : 0\n",
      "  Read/parse errors : 0\n",
      "\n",
      "Example features from first NON-empty file:\n",
      "> 0-vortex_github-actions-dependent-jobs-example_contents_.github_workflows_deploy.Pipeline -> 33 features | sample: ['eq:jobs.build.runs-on==ubuntu-latest', \"eq:jobs.build.steps.[].if==github.ref == 'refs/heads/main' && github.event_name == 'push'\", 'eq:jobs.build.steps.[].name==Checkout repository', 'eq:jobs.build.steps.[].name==Install dependencies', 'eq:jobs.build.steps.[].name==Install npm@7', 'eq:jobs.build.steps.[].name==Release', 'eq:jobs.build.steps.[].name==Setup Node', 'eq:jobs.build.steps.[].run==git config --global user.name test2\\ngit config --global user.email \"test@users.noreply.github.com\"\\nnpm run release\\ngit status\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction without normalization\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Set, Tuple\n",
    "import itertools\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------ CONFIG ------------\n",
    "INCLUDE_EXISTS_FEATURES = True\n",
    "INCLUDE_EQ_FEATURES     = True\n",
    "\n",
    "# Lists use a wildcard \"[]\" segment meaning \"exists in ANY element\"\n",
    "WILDCARD = \"[]\"\n",
    "\n",
    "# ----- YAML loader that avoids \"on:\" -> True -----\n",
    "class NoBoolSafeLoader(yaml.SafeLoader):\n",
    "    pass\n",
    "\n",
    "for ch, mappings in list(NoBoolSafeLoader.yaml_implicit_resolvers.items()):\n",
    "    new_mappings = []\n",
    "    for tag, rx in mappings:\n",
    "        if tag != 'tag:yaml.org,2002:bool':\n",
    "            new_mappings.append((tag, rx))\n",
    "    NoBoolSafeLoader.yaml_implicit_resolvers[ch] = new_mappings\n",
    "\n",
    "def _yaml_load(text: str) -> Any:\n",
    "    try:\n",
    "        return yaml.load(text, Loader=NoBoolSafeLoader)\n",
    "    except Exception:\n",
    "        return None  # treat as unparsable\n",
    "\n",
    "# ----- PURE DFS FEATURE EXTRACTION (NO NORMALIZATION) -----\n",
    "def _walk(node: Any, prefix: List[str], out_exists: Set[str], out_eq: Set[str]):\n",
    "    \"\"\"\n",
    "    DFS over YAML:\n",
    "      - emits exists:path for every visited path\n",
    "      - emits eq:path==value for scalar values (raw, no normalization)\n",
    "    \"\"\"\n",
    "    if isinstance(node, dict):\n",
    "        for k, v in node.items():\n",
    "            key = str(k)  # no strip/lower/etc.\n",
    "            p2 = prefix + [key]\n",
    "            out_exists.add(\"exists:\" + \".\".join(p2))\n",
    "            _walk(v, p2, out_exists, out_eq)\n",
    "\n",
    "    elif isinstance(node, list):\n",
    "        # list-level wildcard path\n",
    "        p2 = prefix + [WILDCARD]\n",
    "        out_exists.add(\"exists:\" + \".\".join(p2))\n",
    "        for v in node:\n",
    "            _walk(v, p2, out_exists, out_eq)\n",
    "\n",
    "    else:\n",
    "        # scalar value (string, int, float, bool, None, etc.)\n",
    "        path = \".\".join(prefix) if prefix else \"\"\n",
    "        if path:\n",
    "            out_exists.add(\"exists:\" + path)\n",
    "            # raw value as string (no length limit / lowercasing)\n",
    "            val = str(node)\n",
    "            out_eq.add(f\"eq:{path}=={val}\")\n",
    "\n",
    "def extract_features(text: str) -> Tuple[Set[str], Set[str]]:\n",
    "    data = _yaml_load(text)\n",
    "    if data is None:\n",
    "        return set(), set()\n",
    "    ex, eq = set(), set()\n",
    "    _walk(data, [], ex, eq)\n",
    "    return ex, eq\n",
    "\n",
    "def process_file(p: Path) -> Tuple[str, Set[str], bool]:\n",
    "    # Runs in worker. Returns (filepath, feature_set, had_error_flag)\n",
    "    try:\n",
    "        txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return str(p), set(), True\n",
    "    ex, eq = extract_features(txt)\n",
    "    feats = set()\n",
    "    if INCLUDE_EXISTS_FEATURES:\n",
    "        feats |= ex\n",
    "    if INCLUDE_EQ_FEATURES:\n",
    "        feats |= eq\n",
    "    return str(p), feats, False\n",
    "\n",
    "# ---- runner that prefers processes and falls back to threads ----\n",
    "def run_extraction(all_files: List[Path]):\n",
    "    import os\n",
    "    extraction: List[Tuple[str, Set[str]]] = []\n",
    "    parse_errors = 0\n",
    "    print(f\"Extracting features from {len(all_files):,} files...\")\n",
    "\n",
    "    try:\n",
    "        import multiprocessing as mp\n",
    "        try:\n",
    "            mp.set_start_method(\"fork\", force=True)\n",
    "        except RuntimeError:\n",
    "            pass  # already set; OK\n",
    "        N_WORKERS = max((os.cpu_count() or 2) - 1, 1)\n",
    "        with mp.Pool(processes=N_WORKERS) as pool:\n",
    "            for fn, feats, err in tqdm(\n",
    "                pool.imap(process_file, all_files, chunksize=64),\n",
    "                total=len(all_files), desc=\"Extracting\"\n",
    "            ):\n",
    "                extraction.append((fn, feats))\n",
    "                if err:\n",
    "                    parse_errors += 1\n",
    "        used = f\"processes (n={N_WORKERS})\"\n",
    "    except Exception as e:\n",
    "        from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "        print(f\"[warn] Process pool failed ({type(e).__name__}: {e}); falling back to threads.\")\n",
    "        max_workers = max(8, (os.cpu_count() or 8) * 4)\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as exr:\n",
    "            futs = {exr.submit(process_file, p): p for p in all_files}\n",
    "            for fut in tqdm(as_completed(futs), total=len(futs), desc=\"Extracting\"):\n",
    "                fn, feats, err = fut.result()\n",
    "                extraction.append((fn, feats))\n",
    "                if err:\n",
    "                    parse_errors += 1\n",
    "        used = f\"threads (n≈{max_workers})\"\n",
    "\n",
    "    # Diagnostics\n",
    "    nonempty = [(fn, f) for fn, f in extraction if f]\n",
    "    empty    = len(extraction) - len(nonempty)\n",
    "    print(f\"\\nExtraction summary [{used}]:\")\n",
    "    print(f\"  Total files       : {len(extraction):,}\")\n",
    "    print(f\"  Non-empty feature : {len(nonempty):,}\")\n",
    "    print(f\"  Empty feature     : {empty:,}\")\n",
    "    print(f\"  Read/parse errors : {parse_errors:,}\")\n",
    "\n",
    "    print(\"\\nExample features from first NON-empty file:\")\n",
    "    if nonempty:\n",
    "        fn, feats = nonempty[0]\n",
    "        print(\">\", Path(fn).name, \"->\", len(feats), \"features | sample:\",\n",
    "              list(itertools.islice(sorted(feats), 8)))\n",
    "    else:\n",
    "        print(\"(!) All empty — check INCLUDE_* flags, DATA_DIR, and YAML validity.\")\n",
    "\n",
    "    return extraction\n",
    "\n",
    "# ---- main entry point ----\n",
    "if __name__ == \"__main__\":\n",
    "    # Dataset location\n",
    "    DATA_DIR = Path(\"/Users/ankita/Desktop/V3_DatasetTest_Valid\")\n",
    "    GLOB_PATTERN = \"**/*.Pipeline\"\n",
    "\n",
    "    all_files = sorted(DATA_DIR.glob(GLOB_PATTERN))\n",
    "    extraction = run_extraction(all_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sPoA5rRZPdMe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from 29,069 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|███████████████████████| 29069/29069 [00:12<00:00, 2366.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extraction summary [processes (n=7)]:\n",
      "  Total files       : 29,069\n",
      "  Non-empty feature : 29,069\n",
      "  Empty feature     : 0\n",
      "  Read/parse errors : 0\n",
      "\n",
      "Example features from first NON-empty file:\n",
      "• 0-vortex_github-actions-dependent-jobs-example_contents_.github_workflows_deploy.Pipeline → 32 features | sample: ['eq:jobs.build.runs-on==ubuntu-latest', \"eq:jobs.build.steps.[].if==github.ref == 'refs/heads/main' && github.event_name == 'push'\", 'eq:jobs.build.steps.[].name==checkout repository', 'eq:jobs.build.steps.[].name==install dependencies', 'eq:jobs.build.steps.[].name==install npm@7', 'eq:jobs.build.steps.[].name==release', 'eq:jobs.build.steps.[].name==setup node', 'eq:jobs.build.steps.[].run==npm ci\\nnpm audit --production\\nnpm test']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Emits two feature types:\n",
    "#   - exists:path\n",
    "#   - eq:path==value\n",
    "# We use a YAML loader that avoids \"on:\" -> True coercion.\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Set, Tuple, Optional\n",
    "import itertools\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------ CONFIG (must be True, or you'll get 0 features) ------------\n",
    "INCLUDE_EXISTS_FEATURES = True\n",
    "INCLUDE_EQ_FEATURES     = True\n",
    "\n",
    "# Value normalization knobs\n",
    "KEEP_NUMERIC_VALUES = False\n",
    "LOWERCASE_VALUES    = True\n",
    "MAX_VALUE_LEN       = 80\n",
    "\n",
    "# Lists use a wildcard \"[]\" segment meaning \"exists in ANY element\"\n",
    "WILDCARD = \"[]\"\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# YAML loader that prevents YAML 1.1 bool coercion (so \"on:\" won't become True)\n",
    "class NoBoolSafeLoader(yaml.SafeLoader):\n",
    "    pass\n",
    "\n",
    "for ch, mappings in list(NoBoolSafeLoader.yaml_implicit_resolvers.items()):\n",
    "    NoBoolSafeSafe = []\n",
    "    for tag, rx in mappings:\n",
    "        if tag != 'tag:yaml.org,2002:bool':\n",
    "            NoBoolSafeSafe.append((tag, rx))\n",
    "    NoBoolSafeLoader.yaml_implicit_resolvers[ch] = NoBoolSafeSafe\n",
    "\n",
    "def _yaml_load(text: str) -> Any:\n",
    "    try:\n",
    "        return yaml.load(text, Loader=NoBoolSafeLoader)\n",
    "    except Exception:\n",
    "        return None  # treat as unparsable\n",
    "\n",
    "def _norm_key(k: Any) -> str:\n",
    "    return k.strip() if isinstance(k, str) else str(k)\n",
    "\n",
    "def _norm_value(v: Any) -> Optional[str]:\n",
    "    if v is None:\n",
    "        return \"null\"\n",
    "    if isinstance(v, bool):\n",
    "        return \"true\" if v else \"false\"\n",
    "    if isinstance(v, (int, float)) and KEEP_NUMERIC_VALUES:\n",
    "        return str(v)\n",
    "    if isinstance(v, str):\n",
    "        s = v.strip()\n",
    "        if LOWERCASE_VALUES:\n",
    "            s = s.lower()\n",
    "        if 0 < len(s) <= MAX_VALUE_LEN:\n",
    "            return s\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def _walk(node: Any, prefix: List[str], out_exists: Set[str], out_eq: Set[str]):\n",
    "    # DFS over YAML; emit exists:* for every path and eq:* for scalars with kept values\n",
    "    if isinstance(node, dict):\n",
    "        for k, v in node.items():\n",
    "            key = _norm_key(k)\n",
    "            p2 = prefix + [key]\n",
    "            out_exists.add(\"exists:\" + \".\".join(p2))\n",
    "            _walk(v, p2, out_exists, out_eq)\n",
    "    elif isinstance(node, list):\n",
    "        p2 = prefix + [WILDCARD]\n",
    "        out_exists.add(\"exists:\" + \".\".join(p2))\n",
    "        for v in node:\n",
    "            _walk(v, p2, out_exists, out_eq)\n",
    "    else:\n",
    "        path = \".\".join(prefix) if prefix else \"\"\n",
    "        if path:\n",
    "            out_exists.add(\"exists:\" + path)\n",
    "        val = _norm_value(node)\n",
    "        if path and val is not None:\n",
    "            out_eq.add(f\"eq:{path}=={val}\")\n",
    "\n",
    "def extract_features(text: str) -> Tuple[Set[str], Set[str]]:\n",
    "    data = _yaml_load(text)\n",
    "    if data is None:\n",
    "        return set(), set()\n",
    "    ex, eq = set(), set()\n",
    "    _walk(data, [], ex, eq)\n",
    "    return ex, eq\n",
    "\n",
    "def process_file(p: Path) -> Tuple[str, Set[str], bool]:\n",
    "    # Runs in worker. Returns (filepath, feature_set, had_error_flag)\n",
    "    try:\n",
    "        txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return str(p), set(), True\n",
    "    ex, eq = extract_features(txt)\n",
    "    feats = set()\n",
    "    if INCLUDE_EXISTS_FEATURES:\n",
    "        feats |= ex\n",
    "    if INCLUDE_EQ_FEATURES:\n",
    "        feats |= eq\n",
    "    return str(p), feats, False\n",
    "\n",
    "# ---- runner that prefers processes (fast) and falls back to threads if pickling fails\n",
    "def run_extraction(all_files: List[Path]):\n",
    "    import os\n",
    "    extraction: List[Tuple[str, Set[str]]] = []\n",
    "    parse_errors = 0\n",
    "    print(f\"Extracting features from {len(all_files):,} files...\")\n",
    "\n",
    "    # Try processes with a start method that avoids spawn pickle issues\n",
    "    try:\n",
    "        import multiprocessing as mp\n",
    "        # 'fork' avoids pickling the function on Unix/macOS; if not available, stays default\n",
    "        try:\n",
    "            mp.set_start_method(\"fork\", force=True)\n",
    "        except RuntimeError:\n",
    "            pass  # already set; OK\n",
    "        N_WORKERS = max((os.cpu_count() or 2) - 1, 1)\n",
    "        with mp.Pool(processes=N_WORKERS) as pool:\n",
    "            for fn, feats, err in tqdm(\n",
    "                pool.imap(process_file, all_files, chunksize=64),\n",
    "                total=len(all_files), desc=\"Extracting\"\n",
    "            ):\n",
    "                extraction.append((fn, feats))\n",
    "                if err:\n",
    "                    parse_errors += 1\n",
    "        used = f\"processes (n={N_WORKERS})\"\n",
    "    except Exception as e:\n",
    "        # If anything about Pool/pickling fails, fallback to threads (I/O-bound is fine)\n",
    "        from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "        print(f\"[warn] Process pool failed ({type(e).__name__}: {e}); falling back to threads.\")\n",
    "        max_workers = max(8, (os.cpu_count() or 8) * 4)\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as exr:\n",
    "            futs = {exr.submit(process_file, p): p for p in all_files}\n",
    "            for fut in tqdm(as_completed(futs), total=len(futs), desc=\"Extracting\"):\n",
    "                fn, feats, err = fut.result()\n",
    "                extraction.append((fn, feats))\n",
    "                if err:\n",
    "                    parse_errors += 1\n",
    "        used = f\"threads (n≈{max_workers})\"\n",
    "\n",
    "    # Diagnostics\n",
    "    nonempty = [(fn, f) for fn, f in extraction if f]\n",
    "    empty    = len(extraction) - len(nonempty)\n",
    "    print(f\"\\nExtraction summary [{used}]:\")\n",
    "    print(f\"  Total files       : {len(extraction):,}\")\n",
    "    print(f\"  Non-empty feature : {len(nonempty):,}\")\n",
    "    print(f\"  Empty feature     : {empty:,}\")\n",
    "    print(f\"  Read/parse errors : {parse_errors:,}\")\n",
    "\n",
    "    print(\"\\nExample features from first NON-empty file:\")\n",
    "    if nonempty:\n",
    "        fn, feats = nonempty[0]\n",
    "        print(\"•\", Path(fn).name, \"→\", len(feats), \"features | sample:\",\n",
    "              list(itertools.islice(sorted(feats), 8)))\n",
    "    else:\n",
    "        print(\"(!) All empty — check INCLUDE_* flags, DATA_DIR, and YAML validity.\")\n",
    "\n",
    "    return extraction\n",
    "\n",
    "# ---- main entry point ----\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        all_files  # ensure it exists from the previous cell\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"`all_files` is not defined. Run the cell that builds the file list first.\")\n",
    "    extraction = run_extraction(all_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhPYhySXPdXK"
   },
   "source": [
    "Cell 5 — Build transactions, prune vocabulary, boolean matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tXRUvPE7PdhT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs kept: 29,065/29,069\n",
      "Unique features (pruned): 2,529\n",
      "Avg features/doc: 29.4\n",
      "\n",
      "Pruning breakdown (files):\n",
      "  • No extracted features         : 0  → /Users/ankita/Desktop/Thesis-work/pattern_outputs2/pruned_empty_features_names.txt\n",
      "  • All features pruned by vocab  : 4  → /Users/ankita/Desktop/Thesis-work/pattern_outputs2/pruned_after_vocab_names.txt\n",
      "  • Total pruned (union)          : 4  → /Users/ankita/Desktop/Thesis-work/pattern_outputs2/pruned_all_names.txt\n",
      "\n",
      "Example pruned (first 5):\n",
      " - 609880ed07f994e8607e8283b3a868d027c9223b.Pipeline\n",
      " - a7d8b362ee149799766528f8516e0119a764b42b.Pipeline\n",
      " - hassio-addons_workflows_contents_.github_workflows_addon-deploy.Pipeline\n",
      " - rpc-org_CWE-094-test_contents_.github_workflows_comment_issue.Pipeline\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "N_DOCS = len(extraction)\n",
    "\n",
    "# --- document-frequency of features ---\n",
    "df_counter = Counter()\n",
    "for _, feats in extraction:\n",
    "    df_counter.update(set(feats))\n",
    "\n",
    "min_df = int(MIN_DOC_FREQ)\n",
    "max_df = int(MAX_DOC_FREQ_RATIO * max(1, N_DOCS))\n",
    "\n",
    "# --- build vocab with pruning ---\n",
    "vocab = [f for f, c in df_counter.items() if c >= min_df and c <= max_df]\n",
    "if TOP_K_FEATURES:\n",
    "    top = sorted(df_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    keep = set(vocab)\n",
    "    vocab = [f for f, _ in top if f in keep][:TOP_K_FEATURES]\n",
    "vocab = sorted(vocab)\n",
    "v_index = {f: i for i, f in enumerate(vocab)}\n",
    "v_set = set(v_index)  # speed up membership checks\n",
    "\n",
    "# --- build transactions (kept docs) ---\n",
    "transactions = []\n",
    "doc_index_to_file = []\n",
    "for fn, feats in extraction:\n",
    "    inter = [f for f in feats if f in v_set]\n",
    "    if inter:\n",
    "        transactions.append(inter)\n",
    "        doc_index_to_file.append(fn)\n",
    "\n",
    "# --- identify pruned files (and why) ---\n",
    "# 1) No features extracted at all (likely parse/empty or extraction knobs filtered everything)\n",
    "pruned_empty_features = sorted(fn for fn, feats in extraction if not feats)\n",
    "\n",
    "# 2) Had features, but ALL were pruned away by vocab thresholds\n",
    "pruned_after_vocab = sorted(\n",
    "    fn for fn, feats in extraction\n",
    "    if feats and not any((f in v_set) for f in feats)\n",
    ")\n",
    "\n",
    "# 3) Union: any reason\n",
    "kept_set   = set(doc_index_to_file)\n",
    "all_set    = set(fn for fn, _ in extraction)\n",
    "pruned_all = sorted(all_set - kept_set)\n",
    "\n",
    "assert set(pruned_all) == set(pruned_empty_features) | set(pruned_after_vocab)\n",
    "\n",
    "# --- helper to save lists ---\n",
    "def _write_list(path: Path, items):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for x in items:\n",
    "            f.write(str(x) + \"\\n\")\n",
    "\n",
    "def _basenames(paths):\n",
    "    # keep only filenames, useful when you just want the .Pipeline names\n",
    "    return sorted({Path(p).name for p in paths}, key=str.lower)\n",
    "\n",
    "# --- save outputs (paths + basenames) ---\n",
    "_write_list(SAVE_DIR / \"pruned_empty_features_paths.txt\", pruned_empty_features)\n",
    "_write_list(SAVE_DIR / \"pruned_after_vocab_paths.txt\",  pruned_after_vocab)\n",
    "_write_list(SAVE_DIR / \"pruned_all_paths.txt\",          pruned_all)\n",
    "\n",
    "_write_list(SAVE_DIR / \"pruned_empty_features_names.txt\", _basenames(pruned_empty_features))\n",
    "_write_list(SAVE_DIR / \"pruned_after_vocab_names.txt\",  _basenames(pruned_after_vocab))\n",
    "_write_list(SAVE_DIR / \"pruned_all_names.txt\",          _basenames(pruned_all))\n",
    "\n",
    "# --- summary ---\n",
    "print(f\"Docs kept: {len(transactions):,}/{N_DOCS:,}\")\n",
    "print(f\"Unique features (pruned): {len(vocab):,}\")\n",
    "avg_feats = round(np.mean([len(t) for t in transactions]) if transactions else 0, 2)\n",
    "print(\"Avg features/doc:\", avg_feats)\n",
    "\n",
    "print(\"\\nPruning breakdown (files):\")\n",
    "print(f\"  • No extracted features         : {len(pruned_empty_features):,}  → {SAVE_DIR/'pruned_empty_features_names.txt'}\")\n",
    "print(f\"  • All features pruned by vocab  : {len(pruned_after_vocab):,}  → {SAVE_DIR/'pruned_after_vocab_names.txt'}\")\n",
    "print(f\"  • Total pruned (union)          : {len(pruned_all):,}  → {SAVE_DIR/'pruned_all_names.txt'}\")\n",
    "\n",
    "# Small peek\n",
    "if pruned_all:\n",
    "    print(\"\\nExample pruned (first 5):\")\n",
    "    for p in pruned_all[:5]:\n",
    "        print(\" -\", Path(p).name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8Dmxu1SPdsh"
   },
   "source": [
    "Cell 6 — Mine frequent itemsets (FP-Growth / Closed / Maximal): (≥20% support, length ≥2)\n",
    "\n",
    "1. How it works: We always mine all frequent itemsets with fpgrowth.\n",
    "\n",
    "    If MINING_MODE == \"maximal\", we drop any itemset that has a larger frequent superset.\n",
    "\n",
    "    If MINING_MODE == \"closed\", we drop an itemset only when a larger frequent superset has the same absolute support (so the subset carries no extra information).\n",
    "\n",
    "    If MINING_MODE == \"all\", we return everything as mined.\n",
    "\n",
    "\n",
    "2. when min_support_abs = 5814, that means\n",
    "\n",
    "if we set MIN_SUPPORT_RATIO = 0.20 (20%). With 29,066 docs kept:\n",
    "\n",
    "Multiply: 29,066 × 0.20 = 5,813.2\n",
    "\n",
    "Take ceiling so we don’t under-count: ceil(5,813.2) = 5,814\n",
    "So an itemset must occur in at least 5,814 files to be considered frequent. This only gets extremely common constructs. reduce min_support_retion to get detailed constructs i.e occurring in less files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Imt92k7YPd27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_support_abs = 5813 of 29065 docs (~20%)\n",
      "Mined itemsets (pre-verification): 242\n",
      "Sample: [(frozenset({'exists:name'}), 25504), (frozenset({'exists:on.push'}), 9586), (frozenset({'exists:jobs.build'}), 7067), (frozenset({'exists:jobs.build.steps.[]'}), 7057), (frozenset({'exists:jobs.build.steps'}), 7057)]\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — Mine frequent itemsets (\"all\" | \"maximal\" | \"closed\") without SciPy/TE\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth  # this submodule works with numpy/pandas only\n",
    "\n",
    "\n",
    "N_DOCS = len(transactions)\n",
    "min_supp_abs = max(1, math.ceil(MIN_SUPPORT_RATIO * N_DOCS))\n",
    "print(f\"min_support_abs = {min_supp_abs} of {N_DOCS} docs (~{MIN_SUPPORT_RATIO:.0%})\")\n",
    "\n",
    "results_itemsets = []  # list[(frozenset(str), int)]\n",
    "\n",
    "if N_DOCS == 0:\n",
    "    print(\"No transactions to mine. Check feature extraction / pruning earlier.\")\n",
    "else:\n",
    "    # --- One-hot encode WITHOUT TransactionEncoder (avoids SciPy import) ---\n",
    "    # We build a dense boolean matrix shaped (docs x |vocab|).\n",
    "    # This is straightforward and fast for ~tens of millions of booleans.\n",
    "    V = len(vocab)\n",
    "    arr = np.zeros((N_DOCS, V), dtype=bool)\n",
    "    for i, T in enumerate(transactions):\n",
    "        for f in T:\n",
    "            j = v_index[f]\n",
    "            arr[i, j] = True\n",
    "\n",
    "    df_bool = pd.DataFrame(arr, columns=vocab)\n",
    "\n",
    "    # --- Mine all frequent itemsets via FP-Growth (ratio threshold) ---\n",
    "    freq = fpgrowth(\n",
    "        df_bool,\n",
    "        min_support=MIN_SUPPORT_RATIO,  # ratio (e.g., 0.20)\n",
    "        use_colnames=True,\n",
    "        max_len=MAX_ITEMSET_LEN\n",
    "    )\n",
    "\n",
    "    # Keep only itemsets of desired min length\n",
    "    freq[\"itemset_len\"] = freq[\"itemsets\"].apply(len)\n",
    "    freq = freq[freq[\"itemset_len\"] >= MIN_ITEMSET_LEN].reset_index(drop=True)\n",
    "\n",
    "    # Convert support ratio → absolute support\n",
    "    freq[\"support_abs\"] = (freq[\"support\"] * N_DOCS).round().astype(int)\n",
    "\n",
    "    # --- Optional post-filters ---\n",
    "    mode = str(MINING_MODE).lower()\n",
    "    if mode in (\"maximal\", \"closed\"):\n",
    "        # Drop any set that has a frequent superset (maximal) or a superset with equal support (closed)\n",
    "        itemsets = list(freq[\"itemsets\"])\n",
    "        supports = list(freq[\"support_abs\"])\n",
    "        lens = freq[\"itemset_len\"].tolist()\n",
    "\n",
    "        # Index by length for fewer subset checks\n",
    "        by_len = {}\n",
    "        for idx, L in enumerate(lens):\n",
    "            by_len.setdefault(L, []).append(idx)\n",
    "\n",
    "        lengths = sorted(by_len.keys())\n",
    "        keep = [True] * len(itemsets)\n",
    "\n",
    "        for pos, L in enumerate(lengths):\n",
    "            longer_idxs = [j for L2 in lengths[pos + 1:] for j in by_len[L2]]\n",
    "            for i in by_len[L]:\n",
    "                if not keep[i]:\n",
    "                    continue\n",
    "                A = itemsets[i]\n",
    "                Sa = supports[i]\n",
    "                for j in longer_idxs:\n",
    "                    if not keep[j]:\n",
    "                        continue\n",
    "                    B = itemsets[j]\n",
    "                    if A.issubset(B):\n",
    "                        if mode == \"maximal\":\n",
    "                            keep[i] = False\n",
    "                            break\n",
    "                        else:  # \"closed\"\n",
    "                            if Sa == supports[j]:\n",
    "                                keep[i] = False\n",
    "                                break\n",
    "\n",
    "        freq = freq[keep].reset_index(drop=True)\n",
    "\n",
    "    # Package output: (frozenset(items), absolute_support)\n",
    "    results_itemsets = [\n",
    "        (frozenset(s), int(supp))\n",
    "        for s, supp in zip(freq[\"itemsets\"], freq[\"support_abs\"])\n",
    "    ]\n",
    "\n",
    "print(f\"Mined itemsets (pre-verification): {len(results_itemsets):,}\")\n",
    "print(\"Sample:\", results_itemsets[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top mined singletons:\n",
      " 25504  exists:name\n",
      "  9586  exists:on.push\n",
      "  7604  exists:on.workflow_call\n",
      "  7067  exists:jobs.build\n",
      "  7057  exists:jobs.build.steps.[]\n",
      "  7057  exists:jobs.build.steps\n",
      "  7057  exists:jobs.build.runs-on\n",
      "  7023  exists:on.workflow_call.inputs\n",
      "  6890  exists:on.push.branches\n",
      "  6808  exists:on.push.branches.[]\n",
      "\n",
      "Top mined pairs:\n",
      "  8541  ['exists:name', 'exists:on.push']\n",
      "  7057  ['exists:jobs.build', 'exists:jobs.build.steps.[]']\n",
      "  7057  ['exists:jobs.build.steps', 'exists:jobs.build.steps.[]']\n",
      "  7057  ['exists:jobs.build', 'exists:jobs.build.steps']\n",
      "  7057  ['exists:jobs.build.runs-on', 'exists:jobs.build.steps']\n",
      "  7057  ['exists:jobs.build.runs-on', 'exists:jobs.build.steps.[]']\n",
      "  7057  ['exists:jobs.build', 'exists:jobs.build.runs-on']\n",
      "  7023  ['exists:on.workflow_call', 'exists:on.workflow_call.inputs']\n",
      "  6940  ['exists:jobs.build', 'exists:name']\n",
      "  6931  ['exists:jobs.build.steps.[]', 'exists:name']\n"
     ]
    }
   ],
   "source": [
    "# Show top few mined singletons and pairs to confirm the threshold is correct\n",
    "singletons = [(s, c) for s, c in results_itemsets if len(s) == 1]\n",
    "print(\"Top mined singletons:\")\n",
    "for iset, supp in sorted(singletons, key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"{supp:6d}  {next(iter(iset))}\")\n",
    "\n",
    "pairs = [(s, c) for s, c in results_itemsets if len(s) == 2]\n",
    "print(\"\\nTop mined pairs:\")\n",
    "for iset, supp in sorted(pairs, key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"{supp:6d}  {sorted(list(iset))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 single features (doc frequency):\n",
      " 25504  exists:name\n",
      "  9586  exists:on.push\n",
      "  7604  exists:on.workflow_call\n",
      "  7067  exists:jobs.build\n",
      "  7057  exists:jobs.build.steps\n",
      "  7057  exists:jobs.build.steps.[]\n",
      "  7057  exists:jobs.build.runs-on\n",
      "  7023  exists:on.workflow_call.inputs\n",
      "  6890  exists:on.push.branches\n",
      "  6808  exists:on.push.branches.[]\n",
      "  6770  exists:jobs.build.steps.[].uses\n",
      "  6540  exists:on.pull_request\n",
      "  6538  exists:jobs.build.steps.[].run\n",
      "  6367  exists:jobs.build.steps.[].name\n",
      "  5720  eq:jobs.build.runs-on==ubuntu-latest\n",
      "  5411  exists:jobs.build.steps.[].with\n",
      "  4460  exists:on.pull_request.branches\n",
      "  4425  exists:on.pull_request.branches.[]\n",
      "  3999  exists:on.[]\n",
      "  3942  eq:on.push.branches.[]==main\n",
      "\n",
      "Max singleton support: 25504 / 29065\n",
      "\n",
      "Top pair count among top 200: 8541 / 29065\n",
      "Pair features:\n",
      "  • exists:name\n",
      "  • exists:on.push\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC: top singletons and rough top-pair supports\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# 1) Singleton doc frequencies\n",
    "singleton_df = Counter()\n",
    "for T in transactions:\n",
    "    singleton_df.update(set(T))\n",
    "\n",
    "top_single = sorted(singleton_df.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "print(\"Top 20 single features (doc frequency):\")\n",
    "for f, c in top_single[:20]:\n",
    "    print(f\"{c:6d}  {f}\")\n",
    "print(f\"\\nMax singleton support: {top_single[0][1]} / {len(transactions)}\")\n",
    "\n",
    "# 2) Rough top pairs among the top-K singletons (K=200)\n",
    "K = 200\n",
    "top_feats = [f for f,_ in top_single[:K]]\n",
    "feat_to_idx = {f:i for i,f in enumerate(top_feats)}\n",
    "pair_counts = Counter()\n",
    "\n",
    "for T in transactions:\n",
    "    present = [feat_to_idx[f] for f in T if f in feat_to_idx]\n",
    "    present.sort()\n",
    "    for i, j in itertools.combinations(present, 2):\n",
    "        pair_counts[(i,j)] += 1\n",
    "\n",
    "if pair_counts:\n",
    "    ((i,j), cnt) = pair_counts.most_common(1)[0]\n",
    "    print(f\"\\nTop pair count among top {K}: {cnt} / {len(transactions)}\")\n",
    "    print(\"Pair features:\")\n",
    "    print(\"  •\", top_feats[i])\n",
    "    print(\"  •\", top_feats[j])\n",
    "else:\n",
    "    print(\"\\nNo pairs among the top features (very unlikely).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caR_zGjDPeB5"
   },
   "source": [
    "Cell 7 — Deterministic verifier: structural matching with wildcards\n",
    "\n",
    "Goal: For each candidate itemset (e.g., {eq:jobs[].runs-on==ubuntu-latest, exists:on.push}), verify on raw YAML that all items hold in the same file.\n",
    "\n",
    "Matcher semantics\n",
    "\n",
    "exists:a.b[].c → path a → b is a list → any element has key c (at any depth under that element, following the rest of the path).\n",
    "\n",
    "eq:a.b[].runs-on==ubuntu-latest → at least one element in list b has runs-on equal to ubuntu-latest.\n",
    "\n",
    "For non-list paths, exact path must exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wIpYqc0TPeMk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifier ready.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Deterministic verifier\n",
    "# A file matches a pattern iff EVERY feature in the itemset is satisfied on the parsed YAML.\n",
    "\n",
    "def _path_parts(path: str) -> List[str]:\n",
    "    return [p for p in path.split(\".\") if p]\n",
    "\n",
    "def _match_exists(node: Any, parts: List[str]) -> bool:\n",
    "    if not parts: return True\n",
    "    head, *rest = parts\n",
    "    if head == WILDCARD:\n",
    "        return isinstance(node, list) and any(_match_exists(ch, rest) for ch in node)\n",
    "    if isinstance(node, dict) and head in node:\n",
    "        return _match_exists(node[head], rest)\n",
    "    return False\n",
    "\n",
    "def _match_eq(node: Any, parts: List[str], target: str) -> bool:\n",
    "    if not parts:\n",
    "        return _norm_value(node) == target\n",
    "    head, *rest = parts\n",
    "    if head == WILDCARD:\n",
    "        return isinstance(node, list) and any(_match_eq(ch, rest, target) for ch in node)\n",
    "    if isinstance(node, dict) and head in node:\n",
    "        return _match_eq(node[head], rest, target)\n",
    "    return False\n",
    "\n",
    "def verify_item_on_doc(doc: Any, feature: str) -> bool:\n",
    "    if doc is None: return False\n",
    "    try:\n",
    "        if feature.startswith(\"exists:\"):\n",
    "            return _match_exists(doc, _path_parts(feature[7:]))\n",
    "        if feature.startswith(\"eq:\"):\n",
    "            lhs_rhs = feature[3:]\n",
    "            if \"==\" not in lhs_rhs: return False\n",
    "            path, val = lhs_rhs.split(\"==\", 1)\n",
    "            return _match_eq(doc, _path_parts(path), val)\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def verify_itemset_on_file(p: str, itemset: Iterable[str]) -> bool:\n",
    "    try:\n",
    "        doc = yaml.load(Path(p).read_text(encoding=\"utf-8\", errors=\"ignore\"), Loader=yaml.BaseLoader)\n",
    "    except Exception:\n",
    "        doc = None\n",
    "    return all(verify_item_on_doc(doc, it) for it in itemset)\n",
    "\n",
    "print(\"Verifier ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0tn2iCYPeXO"
   },
   "source": [
    "Cell 8 — Run verification on mined candidates (batched, parallel), save results\n",
    "\n",
    "This produces:\n",
    "\n",
    "verified_patterns.jsonl → each line: {itemset, support_mined, support_verified, files_path}\n",
    "\n",
    "Per-pattern filename lists (pattern_<idx>.txt) for your cross-validation with Java.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Q68qXphmPeiL",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cell 8] Using min_supp_abs = 5813\n",
      "Verifying 242 itemsets over 29,065 files via postings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying (postings): 100%|███████████████████████| 1/1 [00:00<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/ankita/Desktop/Thesis-work/pattern_outputs2/verified_patterns.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern_gid</th>\n",
       "      <th>itemset</th>\n",
       "      <th>size</th>\n",
       "      <th>support_mined</th>\n",
       "      <th>support_verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>exists:name</td>\n",
       "      <td>1</td>\n",
       "      <td>25504</td>\n",
       "      <td>25504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>exists:on.push</td>\n",
       "      <td>1</td>\n",
       "      <td>9586</td>\n",
       "      <td>9586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>exists:name &amp; exists:on.push</td>\n",
       "      <td>2</td>\n",
       "      <td>8541</td>\n",
       "      <td>8541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>exists:on.workflow_call</td>\n",
       "      <td>1</td>\n",
       "      <td>7604</td>\n",
       "      <td>7604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>exists:jobs.build</td>\n",
       "      <td>1</td>\n",
       "      <td>7067</td>\n",
       "      <td>7067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36</td>\n",
       "      <td>exists:jobs.build &amp; exists:jobs.build.runs-on ...</td>\n",
       "      <td>4</td>\n",
       "      <td>7057</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22</td>\n",
       "      <td>exists:jobs.build &amp; exists:jobs.build.steps &amp; ...</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30</td>\n",
       "      <td>exists:jobs.build.runs-on &amp; exists:jobs.build....</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>exists:jobs.build &amp; exists:jobs.build.runs-on ...</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>exists:jobs.build &amp; exists:jobs.build.runs-on ...</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>exists:jobs.build &amp; exists:jobs.build.steps.[]</td>\n",
       "      <td>2</td>\n",
       "      <td>7057</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19</td>\n",
       "      <td>exists:jobs.build.steps &amp; exists:jobs.build.st...</td>\n",
       "      <td>2</td>\n",
       "      <td>7057</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>exists:jobs.build &amp; exists:jobs.build.steps</td>\n",
       "      <td>2</td>\n",
       "      <td>7057</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>26</td>\n",
       "      <td>exists:jobs.build.runs-on &amp; exists:jobs.build....</td>\n",
       "      <td>2</td>\n",
       "      <td>7057</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>27</td>\n",
       "      <td>exists:jobs.build.runs-on &amp; exists:jobs.build....</td>\n",
       "      <td>2</td>\n",
       "      <td>7057</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>28</td>\n",
       "      <td>exists:jobs.build &amp; exists:jobs.build.runs-on</td>\n",
       "      <td>2</td>\n",
       "      <td>7057</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>exists:jobs.build.steps.[]</td>\n",
       "      <td>1</td>\n",
       "      <td>7057</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>exists:jobs.build.steps</td>\n",
       "      <td>1</td>\n",
       "      <td>7057</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>exists:jobs.build.runs-on</td>\n",
       "      <td>1</td>\n",
       "      <td>7057</td>\n",
       "      <td>7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>241</td>\n",
       "      <td>exists:on.workflow_call &amp; exists:on.workflow_c...</td>\n",
       "      <td>2</td>\n",
       "      <td>7023</td>\n",
       "      <td>7023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13</td>\n",
       "      <td>exists:on.workflow_call.inputs</td>\n",
       "      <td>1</td>\n",
       "      <td>7023</td>\n",
       "      <td>7023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15</td>\n",
       "      <td>exists:jobs.build &amp; exists:name</td>\n",
       "      <td>2</td>\n",
       "      <td>6940</td>\n",
       "      <td>6940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>40</td>\n",
       "      <td>exists:jobs.build &amp; exists:jobs.build.runs-on ...</td>\n",
       "      <td>5</td>\n",
       "      <td>6931</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>exists:jobs.build &amp; exists:jobs.build.steps &amp; ...</td>\n",
       "      <td>4</td>\n",
       "      <td>6931</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>37</td>\n",
       "      <td>exists:jobs.build.runs-on &amp; exists:jobs.build....</td>\n",
       "      <td>4</td>\n",
       "      <td>6931</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>38</td>\n",
       "      <td>exists:jobs.build &amp; exists:jobs.build.runs-on ...</td>\n",
       "      <td>4</td>\n",
       "      <td>6931</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>39</td>\n",
       "      <td>exists:jobs.build &amp; exists:jobs.build.runs-on ...</td>\n",
       "      <td>4</td>\n",
       "      <td>6931</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18</td>\n",
       "      <td>exists:jobs.build &amp; exists:jobs.build.steps.[]...</td>\n",
       "      <td>3</td>\n",
       "      <td>6931</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>23</td>\n",
       "      <td>exists:jobs.build.steps &amp; exists:jobs.build.st...</td>\n",
       "      <td>3</td>\n",
       "      <td>6931</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24</td>\n",
       "      <td>exists:jobs.build &amp; exists:jobs.build.steps &amp; ...</td>\n",
       "      <td>3</td>\n",
       "      <td>6931</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>exists:jobs.build.runs-on &amp; exists:jobs.build....</td>\n",
       "      <td>3</td>\n",
       "      <td>6931</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>34</td>\n",
       "      <td>exists:jobs.build.runs-on &amp; exists:jobs.build....</td>\n",
       "      <td>3</td>\n",
       "      <td>6931</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>35</td>\n",
       "      <td>exists:jobs.build &amp; exists:jobs.build.runs-on ...</td>\n",
       "      <td>3</td>\n",
       "      <td>6931</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>17</td>\n",
       "      <td>exists:jobs.build.steps.[] &amp; exists:name</td>\n",
       "      <td>2</td>\n",
       "      <td>6931</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>21</td>\n",
       "      <td>exists:jobs.build.steps &amp; exists:name</td>\n",
       "      <td>2</td>\n",
       "      <td>6931</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>29</td>\n",
       "      <td>exists:jobs.build.runs-on &amp; exists:name</td>\n",
       "      <td>2</td>\n",
       "      <td>6931</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>41</td>\n",
       "      <td>exists:on.push &amp; exists:on.push.branches</td>\n",
       "      <td>2</td>\n",
       "      <td>6890</td>\n",
       "      <td>6890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6</td>\n",
       "      <td>exists:on.push.branches</td>\n",
       "      <td>1</td>\n",
       "      <td>6890</td>\n",
       "      <td>6890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>47</td>\n",
       "      <td>exists:on.push &amp; exists:on.push.branches &amp; exi...</td>\n",
       "      <td>3</td>\n",
       "      <td>6808</td>\n",
       "      <td>6808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>44</td>\n",
       "      <td>exists:on.push.branches &amp; exists:on.push.branc...</td>\n",
       "      <td>2</td>\n",
       "      <td>6808</td>\n",
       "      <td>6808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pattern_gid                                            itemset  size  \\\n",
       "0             0                                        exists:name     1   \n",
       "1             1                                     exists:on.push     1   \n",
       "2            14                       exists:name & exists:on.push     2   \n",
       "3            12                            exists:on.workflow_call     1   \n",
       "4             2                                  exists:jobs.build     1   \n",
       "5            36  exists:jobs.build & exists:jobs.build.runs-on ...     4   \n",
       "6            22  exists:jobs.build & exists:jobs.build.steps & ...     3   \n",
       "7            30  exists:jobs.build.runs-on & exists:jobs.build....     3   \n",
       "8            31  exists:jobs.build & exists:jobs.build.runs-on ...     3   \n",
       "9            33  exists:jobs.build & exists:jobs.build.runs-on ...     3   \n",
       "10           16     exists:jobs.build & exists:jobs.build.steps.[]     2   \n",
       "11           19  exists:jobs.build.steps & exists:jobs.build.st...     2   \n",
       "12           20        exists:jobs.build & exists:jobs.build.steps     2   \n",
       "13           26  exists:jobs.build.runs-on & exists:jobs.build....     2   \n",
       "14           27  exists:jobs.build.runs-on & exists:jobs.build....     2   \n",
       "15           28      exists:jobs.build & exists:jobs.build.runs-on     2   \n",
       "16            3                         exists:jobs.build.steps.[]     1   \n",
       "17            4                            exists:jobs.build.steps     1   \n",
       "18            5                          exists:jobs.build.runs-on     1   \n",
       "19          241  exists:on.workflow_call & exists:on.workflow_c...     2   \n",
       "20           13                     exists:on.workflow_call.inputs     1   \n",
       "21           15                    exists:jobs.build & exists:name     2   \n",
       "22           40  exists:jobs.build & exists:jobs.build.runs-on ...     5   \n",
       "23           25  exists:jobs.build & exists:jobs.build.steps & ...     4   \n",
       "24           37  exists:jobs.build.runs-on & exists:jobs.build....     4   \n",
       "25           38  exists:jobs.build & exists:jobs.build.runs-on ...     4   \n",
       "26           39  exists:jobs.build & exists:jobs.build.runs-on ...     4   \n",
       "27           18  exists:jobs.build & exists:jobs.build.steps.[]...     3   \n",
       "28           23  exists:jobs.build.steps & exists:jobs.build.st...     3   \n",
       "29           24  exists:jobs.build & exists:jobs.build.steps & ...     3   \n",
       "30           32  exists:jobs.build.runs-on & exists:jobs.build....     3   \n",
       "31           34  exists:jobs.build.runs-on & exists:jobs.build....     3   \n",
       "32           35  exists:jobs.build & exists:jobs.build.runs-on ...     3   \n",
       "33           17           exists:jobs.build.steps.[] & exists:name     2   \n",
       "34           21              exists:jobs.build.steps & exists:name     2   \n",
       "35           29            exists:jobs.build.runs-on & exists:name     2   \n",
       "36           41           exists:on.push & exists:on.push.branches     2   \n",
       "37            6                            exists:on.push.branches     1   \n",
       "38           47  exists:on.push & exists:on.push.branches & exi...     3   \n",
       "39           44  exists:on.push.branches & exists:on.push.branc...     2   \n",
       "\n",
       "    support_mined  support_verified  \n",
       "0           25504             25504  \n",
       "1            9586              9586  \n",
       "2            8541              8541  \n",
       "3            7604              7604  \n",
       "4            7067              7067  \n",
       "5            7057              7057  \n",
       "6            7057              7057  \n",
       "7            7057              7057  \n",
       "8            7057              7057  \n",
       "9            7057              7057  \n",
       "10           7057              7057  \n",
       "11           7057              7057  \n",
       "12           7057              7057  \n",
       "13           7057              7057  \n",
       "14           7057              7057  \n",
       "15           7057              7057  \n",
       "16           7057              7057  \n",
       "17           7057              7057  \n",
       "18           7057              7057  \n",
       "19           7023              7023  \n",
       "20           7023              7023  \n",
       "21           6940              6940  \n",
       "22           6931              6931  \n",
       "23           6931              6931  \n",
       "24           6931              6931  \n",
       "25           6931              6931  \n",
       "26           6931              6931  \n",
       "27           6931              6931  \n",
       "28           6931              6931  \n",
       "29           6931              6931  \n",
       "30           6931              6931  \n",
       "31           6931              6931  \n",
       "32           6931              6931  \n",
       "33           6931              6931  \n",
       "34           6931              6931  \n",
       "35           6931              6931  \n",
       "36           6890              6890  \n",
       "37           6890              6890  \n",
       "38           6808              6808  \n",
       "39           6808              6808  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved strict summary: /Users/ankita/Desktop/Thesis-work/pattern_outputs2/verified_patterns_summary.csv\n",
      "Patterns meeting criteria (size≥1, verified≥5813): 242\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ## Cell 8 — FAST verification via inverted index (no re-parsing YAML), save results, build summaries\n",
    "\n",
    "from pathlib import Path\n",
    "import json, math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _load_jsonl(p: Path):\n",
    "    rows = []\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def _save_jsonl(rows, p: Path):\n",
    "    with p.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "# ---------- compute min_supp_abs ----------\n",
    "if \"min_supp_abs\" not in globals() or min_supp_abs is None:\n",
    "    _N = len(doc_index_to_file) if \"doc_index_to_file\" in globals() else 0\n",
    "    min_supp_abs = max(1, math.ceil(MIN_SUPPORT_RATIO * _N))\n",
    "print(f\"[Cell 8] Using min_supp_abs = {min_supp_abs}\")\n",
    "\n",
    "# ---------- try reload first ----------\n",
    "verified_all = None\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "part_files = sorted(SAVE_DIR.glob(\"verified_patterns_maximal_part_*.jsonl\"))\n",
    "final_file = SAVE_DIR / \"verified_patterns.jsonl\"\n",
    "\n",
    "if part_files or final_file.exists():\n",
    "    verified_all = []\n",
    "    if part_files:\n",
    "        for pf in part_files:\n",
    "            verified_all.extend(_load_jsonl(pf))\n",
    "        print(f\"Reloaded {len(verified_all)} rows from {len(part_files)} part files.\")\n",
    "    else:\n",
    "        verified_all = _load_jsonl(final_file)\n",
    "        print(f\"Reloaded {len(verified_all)} rows from {final_file.name}.\")\n",
    "\n",
    "# ---------- if nothing reloaded, run fast verification ----------\n",
    "if not verified_all:\n",
    "    if \"results_itemsets\" not in globals() or not results_itemsets:\n",
    "        raise RuntimeError(\"No mined itemsets in memory. Re-run Cell 6 first.\")\n",
    "\n",
    "    # Map kept files -> their FULL feature sets from extraction (not pruned)\n",
    "    kept_files = list(doc_index_to_file)                             # order aligned with transactions\n",
    "    fn_to_idx = {fn: i for i, fn in enumerate(kept_files)}\n",
    "    full_feats_per_file = [set() for _ in range(len(kept_files))]\n",
    "\n",
    "    # Build once from Cell 4 extraction\n",
    "    # extraction: list of (filename, feats_full)\n",
    "    src = {fn: feats for (fn, feats) in extraction}\n",
    "    for i, fn in enumerate(kept_files):\n",
    "        full_feats_per_file[i] = src.get(fn, set())\n",
    "\n",
    "    # Collect ALL features that appear in mined itemsets to minimize index size\n",
    "    features_needed = set()\n",
    "    for iset, _supp in results_itemsets:\n",
    "        features_needed.update(iset)\n",
    "\n",
    "    # Build posting lists only for needed features\n",
    "    postings = defaultdict(set)   # feature -> set(indices)\n",
    "    for idx, feats in enumerate(full_feats_per_file):\n",
    "        # only index features we need\n",
    "        for f in feats & features_needed:\n",
    "            postings[f].add(idx)\n",
    "\n",
    "    # Verify by intersection (no YAML reparse)\n",
    "    print(f\"Verifying {len(results_itemsets):,} itemsets over {len(kept_files):,} files via postings...\")\n",
    "    verified_all = []\n",
    "    BATCH_VERIFY = max(100, min(1000, len(results_itemsets)))  # auto-batch size\n",
    "    for i in tqdm(range(0, len(results_itemsets), BATCH_VERIFY), desc=\"Verifying (postings)\"):\n",
    "        chunk = results_itemsets[i:i+BATCH_VERIFY]\n",
    "        batch_out = []\n",
    "        for k, (iset, supp_mined) in enumerate(chunk):\n",
    "            # intersect postings\n",
    "            it = iter(iset)\n",
    "            try:\n",
    "                first = next(it)\n",
    "            except StopIteration:\n",
    "                # empty itemset shouldn't happen, guard anyway\n",
    "                cand = set()\n",
    "            else:\n",
    "                cand = set(postings.get(first, set()))\n",
    "                for f in it:\n",
    "                    cand &= postings.get(f, set())\n",
    "                    if not cand:\n",
    "                        break\n",
    "\n",
    "            # materialize filenames\n",
    "            files = [kept_files[j] for j in sorted(cand)]\n",
    "            batch_out.append({\n",
    "                \"idx\": k,\n",
    "                \"global_idx\": i + k,\n",
    "                \"itemset\": sorted(list(iset)),\n",
    "                \"support_mined\": int(supp_mined),\n",
    "                \"support_verified\": len(files),\n",
    "                \"files\": files\n",
    "            })\n",
    "\n",
    "        # save part\n",
    "        part_path = SAVE_DIR / f\"verified_patterns_part_{i//BATCH_VERIFY:03d}.jsonl\"\n",
    "        _save_jsonl(batch_out, part_path)\n",
    "        verified_all.extend(batch_out)\n",
    "\n",
    "    # save combined\n",
    "    _save_jsonl(verified_all, final_file)\n",
    "    print(\"Saved:\", final_file)\n",
    "\n",
    "# Ensure a global_idx for each row\n",
    "for i, r in enumerate(verified_all):\n",
    "    if \"global_idx\" not in r:\n",
    "        r[\"global_idx\"] = i\n",
    "\n",
    "# ---------- strict summary (size ≥ MIN_ITEMSET_LEN, verified ≥ min_supp_abs) ----------\n",
    "rows = []\n",
    "for r in verified_all:\n",
    "    if (\n",
    "        isinstance(r, dict)\n",
    "        and \"itemset\" in r\n",
    "        and \"support_verified\" in r\n",
    "        and r[\"support_verified\"] is not None\n",
    "        and len(r[\"itemset\"]) >= MIN_ITEMSET_LEN\n",
    "        and r[\"support_verified\"] >= min_supp_abs\n",
    "    ):\n",
    "        rows.append({\n",
    "            \"pattern_gid\": r.get(\"global_idx\", r.get(\"idx\", -1)),\n",
    "            \"itemset\": \" & \".join(r[\"itemset\"]),\n",
    "            \"size\": len(r[\"itemset\"]),\n",
    "            \"support_mined\": int(r.get(\"support_mined\", 0)),\n",
    "            \"support_verified\": int(r[\"support_verified\"]),\n",
    "        })\n",
    "\n",
    "summary_cols = [\"pattern_gid\",\"itemset\",\"size\",\"support_mined\",\"support_verified\"]\n",
    "summary_df = pd.DataFrame(rows, columns=summary_cols)\n",
    "\n",
    "if not summary_df.empty:\n",
    "    summary_df = summary_df.sort_values(\n",
    "        [\"support_verified\",\"size\",\"support_mined\"], ascending=[False, False, False]\n",
    "    ).reset_index(drop=True)\n",
    "    display(summary_df.head(40))\n",
    "    summary_csv = SAVE_DIR / \"verified_patterns_summary.csv\"\n",
    "    summary_df.to_csv(summary_csv, index=False)\n",
    "    print(\"Saved strict summary:\", summary_csv)\n",
    "    print(f\"Patterns meeting criteria (size≥{MIN_ITEMSET_LEN}, verified≥{min_supp_abs}): {len(summary_df):,}\")\n",
    "else:\n",
    "    print(f\"No patterns met the strict thresholds (size≥{MIN_ITEMSET_LEN}, verified≥{min_supp_abs}).\")\n",
    "    # Fallback preview: size ≥ MIN_ITEMSET_LEN, verified ≥ 1\n",
    "    fallback_rows = []\n",
    "    for r in verified_all:\n",
    "        if \"support_verified\" in r and len(r.get(\"itemset\", [])) >= MIN_ITEMSET_LEN and r[\"support_verified\"] >= 1:\n",
    "            fallback_rows.append({\n",
    "                \"pattern_gid\": r.get(\"global_idx\", r.get(\"idx\", -1)),\n",
    "                \"itemset\": \" & \".join(r[\"itemset\"]),\n",
    "                \"size\": len(r[\"itemset\"]),\n",
    "                \"support_mined\": int(r.get(\"support_mined\", 0)),\n",
    "                \"support_verified\": int(r[\"support_verified\"]),\n",
    "            })\n",
    "    fb_df = pd.DataFrame(fallback_rows, columns=summary_cols)\n",
    "    if not fb_df.empty:\n",
    "        fb_df = fb_df.sort_values(\n",
    "            [\"support_verified\",\"size\",\"support_mined\"], ascending=[False, False, False]\n",
    "        ).reset_index(drop=True)\n",
    "        print(\"\\nFallback preview (size≥MIN_ITEMSET_LEN, verified≥1):\")\n",
    "        display(fb_df.head(40))\n",
    "        fb_csv = SAVE_DIR / \"verified_patterns_summary_fallback.csv\"\n",
    "        fb_df.to_csv(fb_csv, index=False)\n",
    "        print(\"Saved fallback:\", fb_csv)\n",
    "    else:\n",
    "        print(\"Even fallback is empty—consider lowering MIN_SUPPORT_RATIO or relaxing pruning.\")\n",
    "\n",
    "# ---------- helper: export filenames for any pattern by global id ----------\n",
    "def export_pattern_filenames_by_gid(pattern_gid: int, out_name: str = None) -> Path:\n",
    "    hit = None\n",
    "    for r in verified_all:\n",
    "        if r.get(\"global_idx\") == pattern_gid:\n",
    "            hit = r; break\n",
    "    if hit is None:\n",
    "        raise KeyError(f\"pattern_gid {pattern_gid} not found\")\n",
    "    label = out_name or f\"id_{pattern_gid:06d}\"\n",
    "    out_path = SAVE_DIR / f\"pattern_{label}.txt\"\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for fn in hit[\"files\"]:\n",
    "            f.write(fn + \"\\n\")\n",
    "    print(f\"Exported {len(hit['files'])} filenames → {out_path}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0tn2iCYPeXO"
   },
   "source": [
    "Cell 9 — verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "r9vWmkojPe3R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported filename lists (filenames only):\n",
      "• id_000092: 3015 files → /Users/ankita/Desktop/Thesis-work/Pattern-Mining/pattern_outputs2/pattern_id_000092.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select patterns and export *filenames only* (one .txt per pattern)\n",
    "# Two ways to select:\n",
    "# 1) By global ids (pattern_gid values from the summary)\n",
    "# 2) By exact itemsets (list of lists of feature strings)\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Tuple, List\n",
    "\n",
    "# ---- OPTION 1: select by ids (EDIT THIS) ----\n",
    "SELECTED_PATTERN_IDS = [92]   # eg. fprgowth all [232, 29] maximal[3] closed []\n",
    "\n",
    "# ---- OPTION 2: select by exact itemsets (EDIT THIS) ----\n",
    "# Example: [[\"eq:jobs[].runs-on==ubuntu-latest\",\"eq:jobs[].steps[].uses==actions/checkout@v4\"]]\n",
    "SELECTED_ITEMSETS: List[List[str]] = [] # eg. [\"exists:on.push.branches\"]\n",
    "\n",
    "# Build a lookup from global_idx -> verified row\n",
    "by_gid = {r[\"global_idx\"]: r for r in verified_all}\n",
    "\n",
    "def _only_pipeline_filenames(paths: List[str]) -> List[str]:\n",
    "    \"\"\"Return unique, sorted base names ending with .Pipeline from a list of paths.\"\"\"\n",
    "    names = []\n",
    "    for fn in paths:\n",
    "        name = Path(fn).name\n",
    "        if name.endswith(\".Pipeline\"):\n",
    "            names.append(name)\n",
    "    # unique + stable order\n",
    "    return sorted(set(names), key=str.lower)\n",
    "\n",
    "def _write_filelist(label: str, files: List[str]):\n",
    "    out_path = SAVE_DIR / f\"pattern_{label}.txt\"\n",
    "    just_names = _only_pipeline_filenames(files)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for name in just_names:\n",
    "            f.write(name + \"\\n\")\n",
    "    return out_path, len(just_names)\n",
    "\n",
    "exported = []\n",
    "\n",
    "# Export by pattern ids\n",
    "for gid in SELECTED_PATTERN_IDS:\n",
    "    if gid not in by_gid:\n",
    "        print(f\"[warn] pattern_gid {gid} not found; skipping\")\n",
    "        continue\n",
    "    label = f\"id_{gid:06d}\"\n",
    "    pth, n = _write_filelist(label, by_gid[gid][\"files\"])\n",
    "    exported.append((label, pth, n))\n",
    "\n",
    "# Export by exact itemsets\n",
    "def _norm_iset(iset: Iterable[str]) -> Tuple[str, ...]:\n",
    "    return tuple(sorted(iset))\n",
    "\n",
    "target_itemsets_norm = set(_norm_iset(x) for x in SELECTED_ITEMSETS)\n",
    "\n",
    "for r in verified_all:\n",
    "    if _norm_iset(r[\"itemset\"]) in target_itemsets_norm:\n",
    "        label = \"iset_\" + \"_\".join([str(abs(hash(x)) % 10**6) for x in r[\"itemset\"]])\n",
    "        pth, n = _write_filelist(label, r[\"files\"])\n",
    "        exported.append((label, pth, n))\n",
    "\n",
    "if exported:\n",
    "    print(\"Exported filename lists (filenames only):\")\n",
    "    for label, pth, n in exported:\n",
    "        print(f\"• {label}: {n} files → {pth}\")\n",
    "else:\n",
    "    print(\"No patterns selected yet. Fill SELECTED_PATTERN_IDS or SELECTED_ITEMSETS and re-run this cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MDOGSVMQGsU"
   },
   "source": [
    "Below Compare was successful for (/Input/sample_jobs_builds_runs-on_steps.pattern):\n",
    "1. File 1: /Users/ankita/Desktop/Thesis-work/pattern_outputs/pattern_id_000029.txt\n",
    "   File 2: /Users/ankita/Desktop/Thesis-work/Generic M2T metrics/Patternmatching_files_all_files_sample_confname_jobs_builds_runs-on_steps.txt\n",
    "\n",
    "Below compare has differences for (/Input/sample_on_push_branches.pattern):\n",
    "1. File 1: /Users/ankita/Desktop/Thesis-work/pattern_outputs/pattern_id_000233.txt\n",
    "   File 2: /Users/ankita/Desktop/Thesis-work/Generic M2T metrics/Patternmatching_files_all_files_sample_on_push_branches.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "y7sLCtGlQG3x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Filename comparison summary ===\n",
      "File 1: /Users/ankita/Desktop/Thesis-work/Pattern-mining/pattern_outputs2/pattern_id_000092.txt\n",
      "File 2: /Users/ankita/Desktop/Thesis-work/Generic M2T metrics/Patternmatching_files_all_files.txt\n",
      "Case sensitive: False\n",
      "Require extension: .Pipeline\n",
      "\n",
      "Total unique in file1: 3015\n",
      "Total unique in file2: 3015\n",
      "Matching count        : 3015\n",
      "Non-matching total    : 0\n",
      "  ├─ Only in file1    : 0\n",
      "  └─ Only in file2    : 0\n",
      "\n",
      "Saved:\n",
      "• Matches           → /Users/ankita/Desktop/Thesis-work/Pattern-Mining/pattern_outputs2/compare_matches.txt\n",
      "• Only in file1     → /Users/ankita/Desktop/Thesis-work/Pattern-Mining/pattern_outputs2/compare_only_in_file1.txt\n",
      "• Only in file2     → /Users/ankita/Desktop/Thesis-work/Pattern-Mining/pattern_outputs2/compare_only_in_file2.txt\n",
      "\n",
      "Top matches (up to 10):\n",
      "0-vortex_github-actions-dependent-jobs-example_contents_.github_workflows_deploy.pipeline\n",
      "00143f50002a05b8faad1fbb93ce3e0d85bde964.pipeline\n",
      "00gxd14g_atomic-red-team-pandas_contents_.github_workflows_python-app.pipeline\n",
      "010312c9029247153390b254849aa9fb5d7eeb5a.pipeline\n",
      "01db31583cac9620d6a3c5766e180f76c0f62de6.pipeline\n",
      "01f8fef2e4e4172cfb87871e6209bcf1514a261e.pipeline\n",
      "0643b4aa7432b0a140bdbf6d7c1114b8a0b09437.pipeline\n",
      "071e2e1a76ab99e99f8792b17718a26b94f8d972.pipeline\n",
      "08730027c4b2aab51fe61c69b65dbf9ac0a133c7.pipeline\n",
      "08cff08a6756c4192afae6750b602903500a78a7.pipeline\n",
      "\n",
      "Top only-in-file1 (up to 10):\n",
      "(none)\n",
      "\n",
      "Top only-in-file2 (up to 10):\n",
      "(none)\n"
     ]
    }
   ],
   "source": [
    "# Compare filenames from given 2 files containing list of filenames\n",
    "# Config — edit these two paths:\n",
    "file1 = \"/Users/ankita/Desktop/Thesis-work/Pattern-mining/pattern_outputs2/pattern_id_000092.txt\"\n",
    "file2 = \"/Users/ankita/Desktop/Thesis-work/Generic M2T metrics/Patternmatching_files_all_files.txt\"\n",
    "\n",
    "# Normalization flags\n",
    "CASE_SENSITIVE = False          # set True if you want exact case matching\n",
    "REQUIRE_EXTENSION = \".Pipeline\" # set None to allow all; else only keep names with this suffix\n",
    "\n",
    "# Output folder (reuses SAVE_DIR if you already defined it; else uses current dir)\n",
    "try:\n",
    "    SAVE_DIR\n",
    "except NameError:\n",
    "    from pathlib import Path\n",
    "    SAVE_DIR = Path(\".\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def _load_names(path, case_sensitive=False, require_ext=None):\n",
    "    names = []\n",
    "    path = Path(path)\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s or s.startswith(\"#\"):\n",
    "                continue\n",
    "            base = Path(s).name  # strip directories if present\n",
    "            if require_ext and not base.endswith(require_ext):\n",
    "                continue\n",
    "            names.append(base if case_sensitive else base.lower())\n",
    "    # unique + stable order for display\n",
    "    uniq = sorted(set(names))\n",
    "    return uniq\n",
    "\n",
    "list1 = _load_names(file1, CASE_SENSITIVE, REQUIRE_EXTENSION)\n",
    "list2 = _load_names(file2, CASE_SENSITIVE, REQUIRE_EXTENSION)\n",
    "\n",
    "set1, set2 = set(list1), set(list2)\n",
    "\n",
    "matches = sorted(set1 & set2)\n",
    "only1  = sorted(set1 - set2)\n",
    "only2  = sorted(set2 - set1)\n",
    "\n",
    "# Counts\n",
    "total1 = len(set1)\n",
    "total2 = len(set2)\n",
    "match_count = len(matches)\n",
    "nonmatch_count = len(only1) + len(only2)\n",
    "\n",
    "print(\"=== Filename comparison summary ===\")\n",
    "print(f\"File 1: {file1}\")\n",
    "print(f\"File 2: {file2}\")\n",
    "print(f\"Case sensitive: {CASE_SENSITIVE}\")\n",
    "print(f\"Require extension: {REQUIRE_EXTENSION if REQUIRE_EXTENSION else 'None'}\\n\")\n",
    "\n",
    "print(f\"Total unique in file1: {total1}\")\n",
    "print(f\"Total unique in file2: {total2}\")\n",
    "print(f\"Matching count        : {match_count}\")\n",
    "print(f\"Non-matching total    : {nonmatch_count}\")\n",
    "print(f\"  ├─ Only in file1    : {len(only1)}\")\n",
    "print(f\"  └─ Only in file2    : {len(only2)}\")\n",
    "\n",
    "# Save results\n",
    "out_matches = SAVE_DIR / \"compare_matches.txt\"\n",
    "out_only1   = SAVE_DIR / \"compare_only_in_file1.txt\"\n",
    "out_only2   = SAVE_DIR / \"compare_only_in_file2.txt\"\n",
    "\n",
    "out_matches.write_text(\"\\n\".join(matches) + (\"\\n\" if matches else \"\"), encoding=\"utf-8\")\n",
    "out_only1.write_text(\"\\n\".join(only1) + (\"\\n\" if only1 else \"\"), encoding=\"utf-8\")\n",
    "out_only2.write_text(\"\\n\".join(only2) + (\"\\n\" if only2 else \"\"), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(f\"• Matches           → {out_matches}\")\n",
    "print(f\"• Only in file1     → {out_only1}\")\n",
    "print(f\"• Only in file2     → {out_only2}\")\n",
    "\n",
    "# Optional: show a quick peek of differences\n",
    "def _peek(lst, n=10):\n",
    "    return \"\\n\".join(lst[:n]) if lst else \"(none)\"\n",
    "\n",
    "print(\"\\nTop matches (up to 10):\")\n",
    "print(_peek(matches))\n",
    "print(\"\\nTop only-in-file1 (up to 10):\")\n",
    "print(_peek(only1))\n",
    "print(\"\\nTop only-in-file2 (up to 10):\")\n",
    "print(_peek(only2))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
